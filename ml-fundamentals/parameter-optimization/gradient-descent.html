
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6.1. Gradient descent &#8212; Machine Learning Open Course</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/drawio.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/youtube.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"></script>
    <script>quizdown.init({"quizdown_js": "https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"});</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.2. Loss function" href="loss-function.html" />
    <link rel="prev" title="6. Parameter optimization" href="parameter-optimization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Open Course</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PREREQUISITES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prerequisites/python-programming-basics.html">
   1. Python programming basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../prerequisites/python-programming-advanced.html">
   2. Python programming advanced
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  FUNDAMENTALS OF MACHINE LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-overview.html">
   3. Machine learning overview (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear-regression.html">
   4. Linear regression (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../logistic-regression.html">
   5. Logistic regression (TBD)
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="parameter-optimization.html">
   6. Parameter optimization
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.1. Gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="loss-function.html">
     6.2. Loss function
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neural-network/neural-network.html">
   7. Neural network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-network/nn-basics.html">
     7.1. Neural network basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-network/nn-hands-on.html">
     7.2. Hands on neural network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-network/nn-implementation.html">
     7.3. Neural network implementation from scratch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-summary.html">
   8. Summary of machine learning fundamentals
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ADVANCED MACHINE LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-advanced/kernel-method.html">
   9. Kernel method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-advanced/model-selection.html">
   10. Model selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-advanced/ensemble-learning.html">
   11. Ensemble learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-advanced/unsupervised-learning.html">
   12. Unsupervised learning (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-advanced/generative-models.html">
   13. Generative models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/dl-overview.html">
   14. Deep learning overview (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/CNN.html">
   15. CNN (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/GAN.html">
   16. GAN (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/RNN.html">
   17. RNN (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/AutoEncoder.html">
   18. Autoencoder (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/LSTM.html">
   19. LSTM (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/NLP.html">
   20. NLP (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/time-series.html">
   21. Time series (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/DQN.html">
   22. DQN (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../deep-learning/dl-summary.html">
   23. Summary of deep learning (TBD)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING PRODUCTIONIZATION
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/intro.html">
   24. Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/overview.html">
   25. Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/problem-framing.html">
   26. Problem framing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/data-engineering.html">
   27. Data engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/model-training-and-evaluation.html">
   28. Model training &amp; evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../machine-learning-productionization/model-deployment.html">
   29. Model deployment
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supporting materials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/pytorch.html">
   30. PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/bamboolib.html">
   31. Bamboolib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/mito.html">
   32. Mito
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/KNN.html">
   33. KNN (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/semi-supervised-learning.html">
   34. Semi-supervised learning (TBD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/unbalanced-problems.html">
   35. Unbalanced problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../supporting-materials/automl.html">
   36. AutoML (TBD)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ASSIGNMENT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../assignments/intro.html">
   37. Assignments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/setup.html">
     37.1. Setup
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/python-programming-basics.html">
     37.2. Python Programming Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/project-plan-template.html">
     37.3. Project Plan​ Template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/machine-learning-productionization/data-engineering.html">
     37.4. Data engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/machine-learning-productionization/counterintuitive-challenges-in-ml-debugging.html">
     37.5. Counterintuitive Challenges in ML Debugging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/machine-learning-productionization/debugging-in-classification.html">
     37.6. Case Study: Debugging in Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../assignments/machine-learning-productionization/debugging-in-regression.html">
     37.7. Case Study: Debugging in Regression
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  OTHERS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../slides/overview.html">
   38. Slides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/open-academy/machine-learning/main?urlpath=tree/open-machine-learning-jupyter-book/ml-fundamentals/parameter-optimization/gradient-descent.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/open-academy/machine-learning/blob/main/open-machine-learning-jupyter-book/ml-fundamentals/parameter-optimization/gradient-descent.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/open-academy/machine-learning/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/open-academy/machine-learning//issues/new?title=Issue%20on%20page%20%2Fml-fundamentals/parameter-optimization/gradient-descent.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/open-academy/machine-learning/edit/main/open-machine-learning-jupyter-book/ml-fundamentals/parameter-optimization/gradient-descent.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/ml-fundamentals/parameter-optimization/gradient-descent.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../../_sources/ml-fundamentals/parameter-optimization/gradient-descent.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective-of-this-session">
   6.1.1. Objective of this session
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#video">
   6.1.2. Video
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-be-playful-to-gain-some-intuition">
   6.1.3. Let’s be playful … to gain some intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-mathematics-to-gain-more-insight">
   6.1.4. Some mathematics … to gain more insight
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract">
     6.1.4.1. Abstract
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivatives">
     6.1.4.2. Partial derivatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivative-with-respect-to-m">
     6.1.4.3. Partial derivative with respect to
     <span class="math notranslate nohighlight">
      \(m\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivative-with-respect-to-b">
     6.1.4.4. Partial derivative with respect to
     <span class="math notranslate nohighlight">
      \(b\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#final-function">
     6.1.4.5. Final function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-to-code">
   6.1.5. Time to code!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-with-gradient-descent">
     6.1.5.1. Linear regression With gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-with-stochastic-gradient-descent">
     6.1.5.2. Linear regression with stochastic gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression-with-gradient-descent">
     6.1.5.3. Logistic regression with gradient descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   6.1.6. Your turn 🚀
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-at-the-frontier-of-machine-learning-research">
   6.1.7. [optional] At the frontier of Machine Learning Research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   6.1.8. Bibliography
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gradient descent</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective-of-this-session">
   6.1.1. Objective of this session
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#video">
   6.1.2. Video
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-be-playful-to-gain-some-intuition">
   6.1.3. Let’s be playful … to gain some intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-mathematics-to-gain-more-insight">
   6.1.4. Some mathematics … to gain more insight
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract">
     6.1.4.1. Abstract
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivatives">
     6.1.4.2. Partial derivatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivative-with-respect-to-m">
     6.1.4.3. Partial derivative with respect to
     <span class="math notranslate nohighlight">
      \(m\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivative-with-respect-to-b">
     6.1.4.4. Partial derivative with respect to
     <span class="math notranslate nohighlight">
      \(b\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#final-function">
     6.1.4.5. Final function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-to-code">
   6.1.5. Time to code!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-with-gradient-descent">
     6.1.5.1. Linear regression With gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-with-stochastic-gradient-descent">
     6.1.5.2. Linear regression with stochastic gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression-with-gradient-descent">
     6.1.5.3. Logistic regression with gradient descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   6.1.6. Your turn 🚀
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-at-the-frontier-of-machine-learning-research">
   6.1.7. [optional] At the frontier of Machine Learning Research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   6.1.8. Bibliography
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="gradient-descent">
<h1><span class="section-number">6.1. </span>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h1>
<section id="objective-of-this-session">
<h2><span class="section-number">6.1.1. </span>Objective of this session<a class="headerlink" href="#objective-of-this-session" title="Permalink to this headline">#</a></h2>
<p>We have already learnt how to use Linear Regression and Logistic Regression models.</p>
<p>The code might seem quite easy and intuitive for you. And you might naturally ask:</p>
<ul class="simple">
<li><p>What’s behind the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> function?</p></li>
<li><p>Why sometimes it takes quite a bit for this <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> function to finish running?</p></li>
</ul>
<p>In this session, you will learn that the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> is the training of ML models,
i.e. tuning of parameters for ML models. And the technique behind is called “Gradient Descent”.</p>
</section>
<section id="video">
<h2><span class="section-number">6.1.2. </span>Video<a class="headerlink" href="#video" title="Permalink to this headline">#</a></h2>
<p>The corresponding video (in Chinese) for this notebook is <a class="reference external" href="https://www.bilibili.com/video/BV1SY4y1G7o9/">👉 available here on Bilibili</a>.
You can (and should) watch the video before diving into the details of gradient descent:</p>
<div class="yt-container">
   <iframe src="//player.bilibili.com/player.html?aid=642485873&cid=764796592&page=1&high_quality=1&danmaku=0" allowfullscreen></iframe>
</div>
</section>
<section id="let-s-be-playful-to-gain-some-intuition">
<h2><span class="section-number">6.1.3. </span>Let’s be playful … to gain some intuition<a class="headerlink" href="#let-s-be-playful-to-gain-some-intuition" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=circle%C2%AEDataset=reg-plane&amp;learningRate=0.00001%C2%AEularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.71864&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=true&amp;xSquared=true&amp;ySquared=true&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">Tensorflow Playground</a></p></li>
<li><p><a class="reference external" href="https://github.com/lilipads/gradient_descent_viz">Gradient Descent Visualization</a></p></li>
<li><p><a class="reference external" href="https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87">Optimization Algorithms Visualization</a></p></li>
</ul>
</section>
<section id="some-mathematics-to-gain-more-insight">
<h2><span class="section-number">6.1.4. </span>Some mathematics … to gain more insight<a class="headerlink" href="#some-mathematics-to-gain-more-insight" title="Permalink to this headline">#</a></h2>
<section id="abstract">
<h3><span class="section-number">6.1.4.1. </span>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">#</a></h3>
<p>The idea behind gradient descent is simple - by gradually tuning parameters, such as slope (<span class="math notranslate nohighlight">\(m\)</span>) and the intercept (<span class="math notranslate nohighlight">\(b\)</span>) in our regression function <span class="math notranslate nohighlight">\(y = mx + b\)</span>, we minimize cost.
By cost, we usually mean some kind of a function that tells us how far off our model predicted result. For regression problems we often use <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">squared</span> <span class="pre">error</span></code> (MSE) cost function. If we use gradient descent for the classification problem, we will have a different set of parameters to tune.</p>
<div class="math notranslate nohighlight">
\[ MSE = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y_i})^2 \quad \textrm{where} \quad \hat{y_i} = mx_i + b \]</div>
<p>Now we have to figure out how to tweak parameters <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to reduce MSE.</p>
</section>
<section id="partial-derivatives">
<h3><span class="section-number">6.1.4.2. </span>Partial derivatives<a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">#</a></h3>
<p>We use partial derivatives to find how each individual parameter affects MSE, so that’s where word <em>partial</em> comes from. In simple words, we take the derivative with respect to <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> <strong>separately</strong>. Take a look at the formula below. It looks almost exactly the same as MSE, but this time we added f(m, b) to it. It essentially changes nothing, except now we can plug <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> numbers into it and calculate the result.</p>
<div class="math notranslate nohighlight">
\[𝑓(𝑚,𝑏)= \frac{1}{n}\sum_{i=1}^{n}(y_i - (mx_i+b))^2\]</div>
<p>This formula (or better say function) is better representation for further calculations of partial derivatives. We can ignore sum for now and what comes before that and focus only on <span class="math notranslate nohighlight">\(y - (mx + b)^2\)</span>.</p>
</section>
<section id="partial-derivative-with-respect-to-m">
<h3><span class="section-number">6.1.4.3. </span>Partial derivative with respect to <span class="math notranslate nohighlight">\(m\)</span><a class="headerlink" href="#partial-derivative-with-respect-to-m" title="Permalink to this headline">#</a></h3>
<p>With respect to <span class="math notranslate nohighlight">\(m\)</span> means we derive parameter <span class="math notranslate nohighlight">\(m\)</span> and basically ignore what is going on with <span class="math notranslate nohighlight">\(b\)</span>, or we can say its 0. To derive with respect to <span class="math notranslate nohighlight">\(m\)</span> we will use chain rule.</p>
<div class="math notranslate nohighlight">
\[ [f(g(x))]' = f'(g(x)) * g(x)' \: - \textrm{chain rule}\]</div>
<p>Chain rule applies when one function sits inside of another. If you’re new to this, you’d be surprised that <span class="math notranslate nohighlight">\(()^2\)</span> is outside function, and <span class="math notranslate nohighlight">\(y-(\boldsymbol{m}x+b)\)</span> sits inside it. So, the chain rule says that we should take a derivative of outside function, keep inside function unchanged and then multiply by derivative of the inside function. Lets write these steps down:</p>
<div class="math notranslate nohighlight">
\[ (y - (mx + b))^2 \]</div>
<ol class="simple">
<li><p>Derivative of <span class="math notranslate nohighlight">\(()^2\)</span> is <span class="math notranslate nohighlight">\(2()\)</span>, same as <span class="math notranslate nohighlight">\(x^2\)</span> becomes <span class="math notranslate nohighlight">\(2x\)</span></p></li>
<li><p>We do nothing with <span class="math notranslate nohighlight">\(y - (mx + b)\)</span>, so it stays the same</p></li>
<li><p>Derivative of <span class="math notranslate nohighlight">\(y - (mx + b)\)</span> with respect to <strong><em>m</em></strong> is <span class="math notranslate nohighlight">\((0 - (x + 0))\)</span> or <span class="math notranslate nohighlight">\(-x\)</span>, because <strong><em>y</em></strong> and <strong><em>b</em></strong> are constants, they become 0, and derivative of <strong><em>mx</em></strong> is <strong><em>x</em></strong></p></li>
</ol>
<p>Multiply all parts we get following: <span class="math notranslate nohighlight">\(2 * (y - (mx+b)) * -x\)</span>.</p>
<p>Looks nicer if we move -x to the left: <span class="math notranslate nohighlight">\(-2x *(y-(mx+b))\)</span>. There we have it. The final version of our derivative is the following:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial m} = \frac{1}{n}\sum_{i=1}^{n}-2x_i(y_i - (mx_i+b))\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\frac{df}{dm}\)</span> means we find partial derivative of function f (we mentioned it earlier) with respect to m. We plug our derivative to the summation and we’re done.</p>
</section>
<section id="partial-derivative-with-respect-to-b">
<h3><span class="section-number">6.1.4.4. </span>Partial derivative with respect to <span class="math notranslate nohighlight">\(b\)</span><a class="headerlink" href="#partial-derivative-with-respect-to-b" title="Permalink to this headline">#</a></h3>
<p>Same rules apply to the derivative with respect to b.</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(()^2\)</span> becomes <span class="math notranslate nohighlight">\(2()\)</span>, same as <span class="math notranslate nohighlight">\(x^2\)</span> becomes <span class="math notranslate nohighlight">\(2x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y - (mx + b)\)</span> stays the same</p></li>
<li><p><span class="math notranslate nohighlight">\(y - (mx + b)\)</span> becomes <span class="math notranslate nohighlight">\((0 - (0 + 1))\)</span> or <span class="math notranslate nohighlight">\(-1\)</span>, because <strong><em>y</em></strong> and <strong><em>mx</em></strong> are constants, they become 0, and derivative of <strong><em>b</em></strong> is 1</p></li>
</ol>
<p>Multiply all the parts together and we get <span class="math notranslate nohighlight">\(-2(y-(mx+b))\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial b} = \frac{1}{n}\sum_{i=1}^{n}-2(y_i - (mx_i+b))\]</div>
</section>
<section id="final-function">
<h3><span class="section-number">6.1.4.5. </span>Final function<a class="headerlink" href="#final-function" title="Permalink to this headline">#</a></h3>
<p>Few details we should discuss before jumping into code:</p>
<ol class="simple">
<li><p>Gradient descent is an iterative process and with each iteration (<span class="math notranslate nohighlight">\(epoch\)</span>) we slightly minimizing MSE, so each time we use our derived functions to update parameters <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>Because it’s iterative, we should choose how many iterations we take, or make algorithm stop when we approach minima of MSE. In other words when algorithm is no longer improving MSE, we know it reached minimum.</p></li>
<li><p>Gradient descent has an additional parameter learning rate (<span class="math notranslate nohighlight">\(lr\)</span>), which helps control how fast or slow algorithm going towards minima of MSE</p></li>
</ol>
<p>That’s about it. So you can already understand that Gradient Descent for the most part is just process of taking derivatives and using them over and over to minimize function.</p>
</section>
</section>
<section id="time-to-code">
<h2><span class="section-number">6.1.5. </span>Time to code!<a class="headerlink" href="#time-to-code" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<section id="linear-regression-with-gradient-descent">
<h3><span class="section-number">6.1.5.1. </span>Linear regression With gradient descent<a class="headerlink" href="#linear-regression-with-gradient-descent" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span> <span class="n">n_iters</span><span class="o">=</span><span class="mi">3000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="n">n_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># init parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># gradient descent</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="p">):</span>
            <span class="c1"># approximate y with linear combination of weights and x, plus bias</span>
            <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

            <span class="c1"># compute gradients</span>
            <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
            <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
            <span class="c1"># update parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">y_predicted</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prostate</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;../../../data/prostate.data&quot;</span><span class="p">)</span>
<span class="n">prostate</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">prostate</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">prostate</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;lpsa&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">prostate</span><span class="p">[</span><span class="s2">&quot;lpsa&quot;</span><span class="p">]</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">regressor</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lr&#39;: 0.0003, &#39;n_iters&#39;: 3000, &#39;weights&#39;: array([0.36114314, 0.15172482, 0.01138062, 0.07103796, 0.10143793,
       0.14812986, 0.09146885, 0.00270041]), &#39;bias&#39;: 0.014542612245156478}
0    -1.470137
1    -1.226722
2    -1.633534
3    -1.145394
4    -1.385705
        ...   
92    0.985388
93    1.125408
94    1.936285
95    1.776223
96    1.680470
Name: lpsa, Length: 97, dtype: float64
</pre></div>
</div>
<img alt="../../_images/gradient-descent_4_1.png" src="../../_images/gradient-descent_4_1.png" />
</div>
</div>
</section>
<section id="linear-regression-with-stochastic-gradient-descent">
<h3><span class="section-number">6.1.5.2. </span>Linear regression with stochastic gradient descent<a class="headerlink" href="#linear-regression-with-stochastic-gradient-descent" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearRegressionWithSGD</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span> <span class="n">n_iters</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="n">n_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># init parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span>
        <span class="c1"># stochastic gradient descent</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="p">):</span>
            <span class="c1"># approximate y with linear combination of weights and x, plus bias</span>
            <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
            
            <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span> <span class="c1"># random sample</span>
        
            <span class="n">Xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">indexes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">indexes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_predicted_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">indexes</span><span class="p">)</span>
            
            <span class="c1"># compute gradients</span>
            <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xs</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_predicted_s</span> <span class="o">-</span> <span class="n">ys</span><span class="p">))</span>
            <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_predicted_s</span> <span class="o">-</span> <span class="n">ys</span><span class="p">)</span>
            <span class="c1"># update parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">y_predicted</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prostate</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;../../../data/prostate.data&quot;</span><span class="p">)</span>
<span class="n">prostate</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">prostate</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">prostate</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;lpsa&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">prostate</span><span class="p">[</span><span class="s2">&quot;lpsa&quot;</span><span class="p">]</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegressionWithSGD</span><span class="p">()</span>

<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">regressor</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lr&#39;: 0.0003, &#39;n_iters&#39;: 5000, &#39;weights&#39;: array([0.44858964, 0.2173008 , 0.00503395, 0.09077799, 0.13211597,
       0.13184214, 0.12441767, 0.00482654]), &#39;bias&#39;: 0.021379227016068723}
0    -1.483454
1    -1.189102
2    -1.570906
3    -1.087019
4    -1.482745
        ...   
92    0.568436
93    0.524477
94    1.567436
95    1.264622
96    1.275740
Name: lpsa, Length: 97, dtype: float64
</pre></div>
</div>
<img alt="../../_images/gradient-descent_7_1.png" src="../../_images/gradient-descent_7_1.png" />
</div>
</div>
</section>
<section id="logistic-regression-with-gradient-descent">
<h3><span class="section-number">6.1.5.3. </span>Logistic regression with gradient descent<a class="headerlink" href="#logistic-regression-with-gradient-descent" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="n">n_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># init parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># gradient descent</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="p">):</span>
            <span class="c1"># approximate y with linear combination of weights and x, plus bias</span>
            <span class="n">linear_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
            <span class="c1"># apply sigmoid function</span>
            <span class="n">y_predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">linear_model</span><span class="p">)</span>

            <span class="c1"># compute gradients</span>
            <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
            <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
            <span class="c1"># update parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">linear_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">linear_model</span><span class="p">)</span>
        <span class="n">y_predicted_cls</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y_predicted</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_predicted_cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">heart</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../../../data/SA_heart.csv&quot;</span><span class="p">)</span>
<span class="n">heart</span><span class="o">.</span><span class="n">famhist</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Present&#39;</span><span class="p">,</span> <span class="s1">&#39;Absent&#39;</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">heart</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;row.names&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">heart</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">heart</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">n_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">perf</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LR classification perf:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">perf</span><span class="p">)</span>

<span class="n">error_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LR classification error rate:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LR classification perf:
 [[88  9]
 [40 16]]
LR classification error rate:
 0.3202614379084967
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="your-turn">
<h2><span class="section-number">6.1.6. </span>Your turn 🚀<a class="headerlink" href="#your-turn" title="Permalink to this headline">#</a></h2>
<p>Modify <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> so that the training will use SGD instead of GD.</p>
</section>
<section id="optional-at-the-frontier-of-machine-learning-research">
<h2><span class="section-number">6.1.7. </span>[optional] At the frontier of Machine Learning Research<a class="headerlink" href="#optional-at-the-frontier-of-machine-learning-research" title="Permalink to this headline">#</a></h2>
<div class="yt-container">
   <iframe src="https://www.youtube.com/embed/mdKjMPmcWjY" allowfullscreen></iframe>
</div>
</section>
<section id="bibliography">
<h2><span class="section-number">6.1.8. </span>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=sDv4f4s2SB8">Gradient Descent, Step-by-Step - StatQuest</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=vMh0zPT0tLI">Stochastic Gradient Descent, Clearly Explained!!! - StatQuest</a></p></li>
<li><p><a class="reference external" href="http://43.142.12.204:12345/05-ML_04-Under-the-Hood.html">http://43.142.12.204:12345/05-ML_04-Under-the-Hood.html</a></p></li>
<li><p><a class="reference external" href="http://43.142.12.204:9999/GradientDescentAnimation.html">http://43.142.12.204:9999/GradientDescentAnimation.html</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "open-academy/machine-learning",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ml-fundamentals/parameter-optimization"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="parameter-optimization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6. </span>Parameter optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="loss-function.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.2. </span>Loss function</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Open Academy<br/>
  
      &copy; Copyright 2022-2022.<br/>
    <div class="extra_footer">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> Text content of this work is licensed under the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>