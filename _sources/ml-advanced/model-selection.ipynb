{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ef91ef",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "\n",
    "\n",
    "## Over-fitting\n",
    "\n",
    "\n",
    "## Bias variance tradeoff\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "## L1 Regularization\n",
    "## L2 Regularization\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "\n",
    "## Dropout\n",
    "\n",
    "### Prediction after dropout\n",
    "\n",
    "Reference:\n",
    "https://datascience.stackexchange.com/questions/44293/how-does-dropout-work-during-testing-in-neural-network\n",
    "\n",
    "![](../../images/model-selection/kUc8r.jpg)\n",
    "\n",
    "\n",
    "During training, p neuron activations (usually, p=0.5, so 50%) are dropped. Doing this at the testing stage is not our goal (the goal is to achieve a better generalization). From the other hand, keeping all activations will lead to an input that is unexpected to the network, more precisely, too high (50% higher) input activations for the following layer.\n",
    "\n",
    "Consider the neurons at the output layer. During training, each neuron usually get activations only from two neurons from the hidden layer (while being connected to four), due to dropout. Now, imagine we finished the training and remove dropout. Now activations of the output neurons will be computed based on four values from the hidden layer. This is likely to put the output neurons in unusual regime, so they will produce too large absolute values, being overexcited.\n",
    "\n",
    "To avoid this, the trick is to multiply the input connections' weights of the last layer by 1-p (so, by 0.5). Alternatively, one can multiply the outputs of the hidden layer by 1-p, which is basically the same.\n",
    "\n",
    "<div hidden>\n",
    "https://github.com/bhattbhavesh91/dropout-walkthrough/\n",
    "\n",
    "https://github.com/Coding-Lane/L2-Regularization\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "Machine Learning Tutorial Python - 20: Bias vs Variance In Machine Learning:\n",
    "https://www.youtube.com/watch?v=B01qMFMAgUQ\n",
    "\n",
    "Bias-Variance Tradeoff, Model Flexibility, Overfitting:\n",
    "https://www.youtube.com/watch?v=T9DEGThjDkI\n",
    "\n",
    "Bias/Variance (C2W1L02):\n",
    "https://www.youtube.com/watch?v=SjQyLhQIXSM\n",
    "\n",
    "Machine Learning Tutorial Python - 17: L1 and L2 Regularization | Lasso, Ridge Regression :\n",
    "https://www.youtube.com/watch?v=VqKq78PVO9g\n",
    "\n",
    "Intuitive Explanation of Ridge / Lasso Regression:\n",
    "https://www.youtube.com/watch?v=9LNpiiKCQUo\n",
    "\n",
    "L1 and L2 Regularization CIS 522 - Deep Learning:\n",
    "https://www.youtube.com/watch?v=OLl2nzOeQ68\n",
    "\n",
    "Thierry Slides Week 6, Week 1\n",
    "\n",
    "Ridge, Lasso, Cross validation\n",
    "\n",
    "Early Stopping, Dropout,\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}