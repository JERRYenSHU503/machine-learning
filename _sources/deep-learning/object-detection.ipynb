{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92db163a",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "\n",
    "## What is object detection?\n",
    "\n",
    "In this chapter we will introduce the object detection problem which can be described in this way: given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n",
    "\n",
    "For example, this screenshot of the example application shows how three objects have been recognized and their positions annotated:\n",
    "\n",
    ":::{figure-md} 01_example\n",
    "<img src=\"../../images/deep-learning/objdet/01_example.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Example of the object detection task\n",
    ":::\n",
    "\n",
    "Object detection has now been widely used in many real-world applications, such as autonomous driving, robot vision, video surveillance, etc. The following image shows the growing number of publications that are associated with “object detection” over the past two decades.\n",
    "\n",
    ":::{figure-md} 02_number_of_pu\n",
    "<img src=\"../../images/deep-learning/objdet/02_number_of_pub.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "The increasing number of publications in object detection from 1998 to 2021\n",
    ":::\n",
    "\n",
    "\n",
    "## Challenges\n",
    "\n",
    "In addition to some common challenges in other computer vision tasks such as objects under different viewpoints, illuminations,and intra-class variations, the challenges in object detection include but are not limited to the following aspects: \n",
    "\n",
    "- object rotation and scale changes (e.g., small objects);\n",
    "- accurate object localization;\n",
    "- dense and occluded object detection;\n",
    "- speed up of detection, etc.\n",
    "\n",
    "## History & classic models\n",
    "\n",
    "Since image classification is a classic task for computer vision, there are several models that are well-performed in the past. We can list them as follows: Faster R-CNN, Mask R-CNN, YOLO, FCOS, DETR. In this part, we will introduce them in order.\n",
    "\n",
    "### Faster R-CNN\n",
    "\n",
    "Faster R-CNN is a single, unified network for object detection that utilizes a region proposal network (RPN) with the CNN model2. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals2. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.\n",
    "\n",
    ":::{figure-md} 03_faster_rcnn\n",
    "<img src=\"../../images/deep-learning/objdet/03_faster_rcnn.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "The structure of Faster RCNN {cite}`faster_structure`\n",
    ":::\n",
    "\n",
    "```{note}\n",
    "We can see the paper [here](https://arxiv.org/pdf/1506.01497.pdf).\n",
    "```\n",
    "\n",
    "#### Code\n",
    "\n",
    "Due to the complexity, here we just introduce the key parts of the model, Region Proposal Network(RPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def rpn_head(featuremap, channel, num_anchors):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        label_logits: fHxfWxNA\n",
    "        box_logits: fHxfWxNAx4\n",
    "    \"\"\"\n",
    "    with argscope(Conv2D, data_format='channels_first',\n",
    "                  kernel_initializer=tf.random_normal_initializer(stddev=0.01)):\n",
    "        hidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu)\n",
    "\n",
    "        label_logits = Conv2D('class', hidden, num_anchors, 1)\n",
    "        box_logits = Conv2D('box', hidden, 4 * num_anchors, 1)\n",
    "        # 1, NA(*4), im/16, im/16 (NCHW)\n",
    "\n",
    "        label_logits = tf.transpose(label_logits, [0, 2, 3, 1])  # 1xfHxfWxNA\n",
    "        label_logits = tf.squeeze(label_logits, 0)  # fHxfWxNA\n",
    "\n",
    "        shp = tf.shape(box_logits)  # 1x(NAx4)xfHxfW\n",
    "        box_logits = tf.transpose(box_logits, [0, 2, 3, 1])  # 1xfHxfWx(NAx4)\n",
    "        box_logits = tf.reshape(box_logits, tf.stack([shp[2], shp[3], num_anchors, 4]))  # fHxfWxNAx4\n",
    "    return label_logits, box_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b222411",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "The complete version of Faster R-CNN can be found [here](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN).\n",
    "```\n",
    "\n",
    "### Mask R-CNN\n",
    "\n",
    "Mask R-CNN is a deep neural network aimed to solve both object detection and instance segmentation problems. In other words, it can separate different objects in an image. You give it an image, it gives you the object bounding boxes, classes and masks. Mask R-CNN was developed on top of Faster R-CNN, a Region-Based Convolutional Neural Network. It is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps.\n",
    "\n",
    ":::{figure-md} 04_mask_rcnn\n",
    "<img src=\"../../images/deep-learning/objdet/04_mask_rcnn.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "The structure of Mask RCNN {cite}`mask_structure`\n",
    ":::\n",
    "\n",
    "```{note}\n",
    "We can see the paper [here](https://arxiv.org/pdf/1703.06870v3.pdf).\n",
    "```\n",
    "\n",
    "#### Code\n",
    "\n",
    "The key parts in Mask RCNN are:\n",
    "\n",
    "- RoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):\n",
    "    \"\"\"\n",
    "    Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes.\n",
    "\n",
    "    Args:\n",
    "        image: NCHW\n",
    "        boxes: nx4, x1y1x2y2\n",
    "        box_ind: (n,)\n",
    "        crop_size (int):\n",
    "    Returns:\n",
    "        n,C,size,size\n",
    "    \"\"\"\n",
    "    assert isinstance(crop_size, int), crop_size\n",
    "    boxes = tf.stop_gradient(boxes)\n",
    "\n",
    "    # TF's crop_and_resize produces zeros on border\n",
    "    if pad_border:\n",
    "        # this can be quite slow\n",
    "        image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode='SYMMETRIC')\n",
    "        boxes = boxes + 1\n",
    "\n",
    "    @under_name_scope()\n",
    "    def transform_fpcoor_for_tf(boxes, image_shape, crop_shape):\n",
    "        \"\"\"\n",
    "        The way tf.image.crop_and_resize works (with normalized box):\n",
    "        Initial point (the value of output[0]): x0_box * (W_img - 1)\n",
    "        Spacing: w_box * (W_img - 1) / (W_crop - 1)\n",
    "        Use the above grid to bilinear sample.\n",
    "\n",
    "        However, what we want is (with fpcoor box):\n",
    "        Spacing: w_box / W_crop\n",
    "        Initial point: x0_box + spacing/2 - 0.5\n",
    "        (-0.5 because bilinear sample (in my definition) assumes floating point coordinate\n",
    "         (0.0, 0.0) is the same as pixel value (0, 0))\n",
    "\n",
    "        This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize\n",
    "\n",
    "        Returns:\n",
    "            y1x1y2x2\n",
    "        \"\"\"\n",
    "        x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)\n",
    "\n",
    "        spacing_w = (x1 - x0) / tf.cast(crop_shape[1], tf.float32)\n",
    "        spacing_h = (y1 - y0) / tf.cast(crop_shape[0], tf.float32)\n",
    "\n",
    "        imshape = [tf.cast(image_shape[0] - 1, tf.float32), tf.cast(image_shape[1] - 1, tf.float32)]\n",
    "        nx0 = (x0 + spacing_w / 2 - 0.5) / imshape[1]\n",
    "        ny0 = (y0 + spacing_h / 2 - 0.5) / imshape[0]\n",
    "\n",
    "        nw = spacing_w * tf.cast(crop_shape[1] - 1, tf.float32) / imshape[1]\n",
    "        nh = spacing_h * tf.cast(crop_shape[0] - 1, tf.float32) / imshape[0]\n",
    "\n",
    "        return tf.concat([ny0, nx0, ny0 + nh, nx0 + nw], axis=1)\n",
    "\n",
    "    image_shape = tf.shape(image)[2:]\n",
    "\n",
    "    boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])\n",
    "    image = tf.transpose(image, [0, 2, 3, 1])   # nhwc\n",
    "    ret = tf.image.crop_and_resize(\n",
    "        image, boxes, tf.cast(box_ind, tf.int32),\n",
    "        crop_size=[crop_size, crop_size])\n",
    "    ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss\n",
    "    return ret\n",
    "\n",
    "\n",
    "def roi_align(featuremap, boxes, resolution):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        featuremap: 1xCxHxW\n",
    "        boxes: Nx4 floatbox\n",
    "        resolution: output spatial resolution\n",
    "\n",
    "    Returns:\n",
    "        NxCx res x res\n",
    "    \"\"\"\n",
    "    # sample 4 locations per roi bin\n",
    "    ret = crop_and_resize(\n",
    "        featuremap, boxes,\n",
    "        tf.zeros(tf.shape(boxes)[0], dtype=tf.int32),\n",
    "        resolution * 2)\n",
    "    try:\n",
    "        avgpool = tf.nn.avg_pool2d\n",
    "    except AttributeError:\n",
    "        avgpool = tf.nn.avg_pool\n",
    "    ret = avgpool(ret, [1, 1, 2, 2], [1, 1, 2, 2], padding='SAME', data_format='NCHW')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b212eb",
   "metadata": {},
   "source": [
    "- Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models.\n",
    "        num_category(int):\n",
    "        num_convs (int): number of convolution layers\n",
    "        norm (str or None): either None or 'GN'\n",
    "\n",
    "    Returns:\n",
    "        mask_logits (N x num_category x 2s x 2s):\n",
    "    \"\"\"\n",
    "    assert norm in [None, 'GN'], norm\n",
    "    l = feature\n",
    "    with argscope([Conv2D, Conv2DTranspose], data_format='channels_first',\n",
    "                  kernel_initializer=tfv1.variance_scaling_initializer(\n",
    "                      scale=2.0, mode='fan_out',\n",
    "                      distribution='untruncated_normal')):\n",
    "        # c2's MSRAFill is fan_out\n",
    "        for k in range(num_convs):\n",
    "            l = Conv2D('fcn{}'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)\n",
    "            if norm is not None:\n",
    "                l = GroupNorm('gn{}'.format(k), l)\n",
    "        l = Conv2DTranspose('deconv', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu)\n",
    "        l = Conv2D('conv', l, num_category, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03e6f9",
   "metadata": {},
   "source": [
    "### FCOS\n",
    "\n",
    "FCOS (Fully Convolutional One-Stage Object Detection) is a one-stage object detection algorithm that uses a fully convolutional architecture to detect objects. It is a simple and effective method for object detection that does not require region proposal networks (RPNs) or anchor boxes.\n",
    "\n",
    "<!---\n",
    ":::{figure-md} 05_fcos\n",
    "<img src=\"../../images/deep-learning/objdet/05_fcos.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "The structure of FCOS {cite}`fcos_structure`\n",
    ":::\n",
    "\n",
    "--->\n",
    "\n",
    "```{note}\n",
    "We can see the paper [here](https://arxiv.org/pdf/1904.01355v5.pdf).\n",
    "```\n",
    "\n",
    "#### Code\n",
    "\n",
    "The key point in FCOS is Center-ness, which is a novel way to calculate target: $centerness = \\sqrt{\\frac{min(l,r)}{max(l,r)} \\frac{min(t,b)}{max(t,b)}}$\n",
    "\n",
    ":::{figure-md} 06_centerness\n",
    "<img src=\"../../images/deep-learning/objdet/06_centerness.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of centerness {cite}`fcos_structure`\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CenternessNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_anchor, use_bias = None, concat = False, convolution = conf_conv, normalize = None, activation = tf.keras.activations.sigmoid, **kwargs):\n",
    "        super(CenternessNet, self).__init__(**kwargs)\n",
    "        self.n_anchor = n_anchor\n",
    "        self.use_bias = (normalize is None) if use_bias is None else use_bias\n",
    "        self.concat = concat\n",
    "        self.activation = activation\n",
    "        self.convolution = convolution\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, (tuple, list)):\n",
    "            input_shape = [input_shape]\n",
    "        \n",
    "        self.layers = [self.convolution(self.n_anchor, 3, padding = \"same\", use_bias = self.use_bias, name = \"head\")]\n",
    "        if self.normalize is not None:\n",
    "            self.layers.append(self.normalize(name = \"norm\"))\n",
    "        self.layers.append(tf.keras.layers.Reshape([-1, 1], name = \"reshape\"))\n",
    "        if self.concat and 1 < len(input_shape):\n",
    "            self.post = tf.keras.layers.Concatenate(axis = -2, name = \"logits_concat\")\n",
    "        self.act = tf.keras.layers.Activation(self.activation, dtype = tf.float32, name = \"logits\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not isinstance(inputs, (tuple, list)):\n",
    "            inputs = [inputs]\n",
    "        out = []\n",
    "        for x in inputs:\n",
    "            for l in self.layers:\n",
    "                x = l(x)\n",
    "            out.append(x)\n",
    "        if len(out) == 1:\n",
    "            out = out[0]\n",
    "        elif self.concat:\n",
    "            out = self.post(out)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            out = [self.act(o) for o in out]\n",
    "        else:\n",
    "            out = self.act(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1001d",
   "metadata": {},
   "source": [
    "### DETR\n",
    "\n",
    "Similar to image classification, Transformer is also used in object detection task, and the classical one is DEtection TRansformer(DETR).\n",
    "\n",
    "<!---\n",
    ":::{figure-md} 07_DETR\n",
    "<img src=\"../../images/deep-learning/objdet/07_detr.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of DETR {cite}`detr_structure`\n",
    ":::\n",
    "--->\n",
    "\n",
    "```{note}\n",
    "We can see the paper [here](https://arxiv.org/pdf/2005.12872v3.pdf).\n",
    "```\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "class DETR(tf.keras.Model):\n",
    "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
    "    def __init__(self, \n",
    "                 backbone:      tf.keras.Model, \n",
    "                 transformer:   tf.keras.Model, \n",
    "                 num_classes:   int, \n",
    "                 num_queries:   int, \n",
    "                 aux_loss:      bool = False, \n",
    "                 **kwargs):\n",
    "                 \n",
    "        super(DETR, self).__init__(**kwargs)\n",
    "        self.num_queries = num_queries\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = Dense(num_classes+1, name='class_embed')\n",
    "        self.bbox_embed = MLP(hidden_dim, 4, 3, name='bbox_embed')\n",
    "        self.query_embed = Embedding(num_queries, hidden_dim, name='query_embed')\n",
    "        self.query_embed.build((num_queries, hidden_dim))\n",
    "        self.input_proj = Conv2D(hidden_dim, 1, name='input_proj')\n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "    def call(self, samples: Dict):\n",
    "        features, pos = self.backbone(samples)\n",
    "        src, mask = features[-1][1]['img'], features[-1][1]['mask']\n",
    "        assert mask is not None\n",
    "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weights[0], pos[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cca477",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "The complete version of DETR can be found [here](https://github.com/PaperCodeReview/DETR-TF).\n",
    "```\n",
    "\n",
    "## Your turn! 🚀\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Tensorpack](https://github.com/tensorpack) for creating the open-source project [tensorpack](https://github.com/tensorpack/tensorpack), [Hyungjin Kim](https://github.com/Burf) for creating the open-source project [TFDetection](https://github.com/Burf/TFDetection) and [PaperCodeReview](https://github.com/PaperCodeReview) for creating the open-source project [DETR-TF](https://github.com/PaperCodeReview/DETR-TF). They inspire the majority of the content in this chapter.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"../html/object-detection/demo.html\" width=\"105%\" height=\"400px;\"\n",
    "style=\"border:none;\"></iframe>\n",
    "A demo of object detection. <a href=\"https://github.com/mtschirs/js-objectdetect\">[source]</a>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   70,
   95,
   121,
   210,
   214,
   242,
   271,
   313,
   333,
   366
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}