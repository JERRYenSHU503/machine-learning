{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233e0b00",
   "metadata": {},
   "source": [
    "# Summary of machine learning fundamentals\n",
    "\n",
    "\n",
    "## Machine Learning Landscape : Discriminative Models\n",
    "\n",
    "Most of supervised machine learning can be looked at using the following framework: \n",
    "You have a set of training points $(x_i, y_i)$, and you want to find a function f that \"fits the data well\", \n",
    "that is, $yi \\approx f(x_i)$ for most $i$.\n",
    "\n",
    "You will start by doing the following:\n",
    "\n",
    "- Define the form of $f$. For instance, we can define $f = wx + b$, for some constants $w$ and $b$. \n",
    "Note that this is a set of functions — for different values of $w$ and $b$, \n",
    "you will get different functions $f$, and you want to find an $f$\n",
    "from this set that does the “best”.\n",
    "- As you might have noticed, we have been talking about this notion of “best”, \n",
    "which is ill-defined up to this point. So, we need to make this more concrete. \n",
    "The goal here, as stated above, is to have $y_i \\approx f(x_i)$\n",
    "for most $i$.\n",
    "\n",
    "The above two steps essentially define the **function class** and the **loss function** respectively.\n",
    "\n",
    "Depending on how you choose your function class and the loss function, \n",
    "you get different supervised learning models or even unsupervised learning models:\n",
    "\n",
    "- Linear function class with squared-error loss function — Linear regression\n",
    "- Linear function class with logistic loss function — Logistic regression\n",
    "- Linear function class with hinge loss — SVM\n",
    "- Function class containing a network of neurons with cross-entropy loss — Neural networks\n",
    "\n",
    "and so on.\n",
    "\n",
    "## How to conceive a \"new\" Machine Learning algorithm : Discriminative Models\n",
    "\n",
    "\n",
    "- [How about perpendicular distance instead of vertical distance for Linear Regression?](https://math.stackexchange.com/questions/1530298/variant-of-linear-regression-using-perpendicular-distance-instead-of-vertical)\n",
    "\n",
    "- [How about Logistic Regression with Kernel Trick?](https://www.quora.com/How-can-one-use-kernels-utilizing-the-kernel-trick-in-logistic-regression)\n",
    "\n",
    "- [How about a Neural Network with Kernel Trick]\n",
    "\n",
    "- [How about a Neural Network not by Layers but inter-connected]\n",
    "\n",
    "- [How about horizontal or vertical lines - then we have Decision Tree]\n",
    "\n",
    "- [How about counting numbers - then we have KNN]\n",
    "\n",
    "- [How about counting numbers with Kernel trick](https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour)\n",
    "\n",
    "\n",
    "\n",
    "## Machine Learning Landscape : Generative Models \n",
    "\n",
    "\n",
    "## LDA \n",
    "\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "## Semi-supervised learning\n",
    "\n",
    "\n",
    "<div hidden>\n",
    "    https://www.baeldung.com/cs/svm-vs-neural-network\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}