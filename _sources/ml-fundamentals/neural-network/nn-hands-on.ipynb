{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fff7e4",
   "metadata": {},
   "source": [
    "# Hands on neural network\n",
    "\n",
    "## Neural network with Tensorflow : Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3030edf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_lfw_people\n\u001b[0;32m----> 7\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_lfw_people\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_faces_per_person\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 766 images of 31 * 23 pixel black & white\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(faces\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/site-packages/sklearn/datasets/_lfw.py:321\u001b[0m, in \u001b[0;36mfetch_lfw_people\u001b[0;34m(data_home, funneled, resize, min_faces_per_person, color, slice_, download_if_missing, return_X_y)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_lfw_people\u001b[39m(\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    234\u001b[0m     data_home\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m     return_X_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    242\u001b[0m ):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124;03m\"\"\"Load the Labeled Faces in the Wild (LFW) people dataset \\\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m(classification).\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     lfw_home, data_folder_path \u001b[38;5;241m=\u001b[39m \u001b[43m_check_fetch_lfw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunneled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunneled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_if_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_if_missing\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading LFW people faces from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lfw_home)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# wrap the loader in a memoizing function that will return memmaped data\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# arrays for optimal memory usage\u001b[39;00m\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/site-packages/sklearn/datasets/_lfw.py:111\u001b[0m, in \u001b[0;36m_check_fetch_lfw\u001b[0;34m(data_home, funneled, download_if_missing)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecompressing the data archive to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, data_folder_path)\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr:gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlfw_home\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     remove(archive_path)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lfw_home, data_folder_path\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/tarfile.py:2045\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         tarinfo\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0o700\u001b[39m\n\u001b[1;32m   2044\u001b[0m     \u001b[38;5;66;03m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/tarfile.py:2086\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2083\u001b[0m     tarinfo\u001b[38;5;241m.\u001b[39m_link_target \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mlinkname)\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2086\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorlevel \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/tarfile.py:2159\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[0;32m-> 2159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/tarfile.py:2208\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2206\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2208\u001b[0m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/tarfile.py:247\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    245\u001b[0m blocks, remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(length, bufsize)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(blocks):\n\u001b[0;32m--> 247\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m bufsize:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/gzip.py:300\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/gzip.py:509\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompressed file ended before the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend-of-stream marker was reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_read_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43muncompress\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(uncompress)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m uncompress\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/open-machine-learning-jupyter-book/lib/python3.9/gzip.py:514\u001b[0m, in \u001b[0;36m_GzipReader._add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_read_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc \u001b[38;5;241m=\u001b[39m \u001b[43mzlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrc32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=200, resize=0.25)\n",
    "\n",
    "# 766 images of 31 * 23 pixel black & white\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 different target classes\n",
    "np.unique(faces.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4b16c",
   "metadata": {},
   "source": [
    "Let's visualize some faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ac851",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,10))\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(faces.target_names[faces.target[i]], size=12)\n",
    "    plt.imshow(faces.images[i], cmap=plt.cm.gray)\n",
    "    plt.xticks(()); plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781ebe8",
   "metadata": {},
   "source": [
    "### Minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f63c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten our 766 images\n",
    "X = faces.images.reshape(766, 31*23)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ebd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = faces.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6abba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854842d",
   "metadata": {},
   "source": [
    "### Simple model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_dim=713))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy', \n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aeef7d",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6675a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(scaler.transform(X_test), y_test)\n",
    "# returns [loss, metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475ff06",
   "metadata": {},
   "source": [
    "Is it good? What's our baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bcfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline score\n",
    "530 / (530+236)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6801db",
   "metadata": {},
   "source": [
    "### Let's check our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "model.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932284f",
   "metadata": {},
   "source": [
    "## Linear regression with Tensorflow\n",
    "\n",
    "## Logistic regression with Tensorflow"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "source_map": [
   14,
   21,
   34,
   37,
   41,
   48,
   52,
   60,
   68,
   77,
   82,
   86,
   100,
   107,
   112,
   115,
   122,
   130,
   133,
   140,
   143
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}