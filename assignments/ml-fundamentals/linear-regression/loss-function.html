
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>37.48. Loss Function &#8212; Ocademy Open Machine Learning Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/drawio.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/youtube.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "ocademy-ai/machine-learning-utterances");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"></script>
    <script>quizdown.init({"quizdown_js": "https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"});</script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="37.49. Gradient descent" href="gradient-descent.html" />
    <link rel="prev" title="37.47. Linear Regression Metrics" href="linear-regression-metrics.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint">Learn AI together, for free! At <a color='lightblue' href='https://ocademy.cc'><u style='color:lightblue;'>Ocademy</u></a>.</div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo-long.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Ocademy Open Machine Learning Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PREREQUISITES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../prerequisites/python-programming-introduction.html">
   1. Python programming introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../prerequisites/python-programming-basics.html">
   2. Python programming basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../prerequisites/python-programming-advanced.html">
   3. Python programming advanced
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DATA SCIENCE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../data-science/introduction/introduction.html">
   4. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/introduction/defining-data-science.html">
     4.1. Defining data science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/introduction/data-science-ethics.html">
     4.2. Data Science ethics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/introduction/defining-data.html">
     4.3. Defining data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/introduction/introduction-to-statistics-and-probability.html">
     4.4. Introduction to statistics and probability
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../data-science/working-with-data/working-with-data.html">
   5. Working with data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/working-with-data/relational-databases.html">
     5.1. Relational databases
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/working-with-data/non-relational-data.html">
     5.2. Non-relational data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/working-with-data/numpy.html">
     5.3. NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/working-with-data/pandas.html">
     5.4. Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/working-with-data/data-preparation.html">
     5.5. Data preparation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../data-science/data-visualization/data-visualization.html">
   6. Data visualization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-visualization/visualization-distributions.html">
     6.1. Visualizing distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-visualization/visualization-proportions.html">
     6.2. Visualizing proportions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-visualization/visualization-relationships.html">
     6.3. Visualizing relationships: all about honey 🍯
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-visualization/meaningful-visualizations.html">
     6.4. Making meaningful visualizations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../data-science/data-science-lifecycle/data-science-lifecycle.html">
   7. Data Science lifecycle
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-lifecycle/introduction.html">
     7.1. Introduction to the Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-lifecycle/analyzing.html">
     7.2. Analyzing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-lifecycle/communication.html">
     7.3. Communication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../data-science/data-science-in-the-cloud/data-science-in-the-cloud.html">
   8. Data Science in the cloud
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-in-the-cloud/introduction.html">
     8.1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-in-the-cloud/the-low-code-no-code-way.html">
     8.2. The “low code/no code” way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../data-science/data-science-in-the-cloud/the-azure-ml-sdk-way.html">
     8.3. Data Science in the cloud: The “Azure ML SDK” way
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data-science/data-science-in-the-wild.html">
   9. Data Science in the real world
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING BASICS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml-fundamentals/ml-overview.html">
   10. Machine Learning overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml-fundamentals/regression/regression-models-for-machine-learning.html">
   11. Regression models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/regression/tools-of-the-trade.html">
     11.1. Tools of the trade
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/regression/managing-data.html">
     11.2. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/regression/linear-and-polynomial-regression.html">
     11.3. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/regression/logistic-regression.html">
     11.4. Logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml-fundamentals/build-a-web-app-to-use-a-machine-learning-model.html">
   12. Build a web app to use a Machine Learning model
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml-fundamentals/classification/getting-started-with-classification.html">
   13. Getting started with classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/classification/introduction-to-classification.html">
     13.1. Introduction to classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/classification/more-classifiers.html">
     13.2. More classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/classification/yet-other-classifiers.html">
     13.3. Yet other classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-fundamentals/classification/applied-ml-build-a-web-app.html">
     13.4. Applied Machine Learning : build a web app
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ADVANCED MACHINE LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml-advanced/clustering/clustering-models-for-machine-learning.html">
   14. Clustering models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/clustering/introduction-to-clustering.html">
     14.1. Introduction to clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/clustering/k-means-clustering.html">
     14.2. K-Means clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml-advanced/ensemble-learning/getting-started-with-ensemble-learning.html">
   15. Getting started with ensemble learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/ensemble-learning/bagging.html">
     15.1. Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/ensemble-learning/random-forest.html">
     15.2. Random forest
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/ensemble-learning/feature-importance.html">
     15.3. Feature importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml-advanced/gradient-boosting/introduction-to-gradient-boosting.html">
   16. Introduction to Gradient Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/gradient-boosting/gradient-boosting.html">
     16.1. Gradient Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/gradient-boosting/gradient-boosting-example.html">
     16.2. Gradient boosting example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/gradient-boosting/xgboost.html">
     16.3. XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml-advanced/gradient-boosting/xgboost-k-fold-cv-feature-importance.html">
     16.4. XGBoost + k-fold CV + Feature Importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml-advanced/unsupervised-learning.html">
   17. Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml-advanced/kernel-method.html">
   18. Kernel method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml-advanced/model-selection.html">
   19. Model selection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/dl-overview.html">
   20. Intro to Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/cnn.html">
   21. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/gan.html">
   22. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/rnn.html">
   23. Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/autoencoder.html">
   24. Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/lstm.html">
   25. Long-short term memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/time-series.html">
   26. Time series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/dqn.html">
   27. Deep Q-learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/image-classification.html">
   28. Image classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/image-segmentation.html">
   29. Image segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/difussion-model.html">
   30. Diffusion Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deep-learning/object-detection.html">
   31. Object detection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING OPERATIONS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../machine-learning-productionization/overview.html">
   32. Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../machine-learning-productionization/problem-framing.html">
   33. Problem framing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../machine-learning-productionization/data-engineering.html">
   34. Data engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../machine-learning-productionization/model-training-and-evaluation.html">
   35. Model training &amp; evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../machine-learning-productionization/model-deployment.html">
   36. Model deployment
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  OTHERS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../README.html">
   37. Self-paced assignments
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../../set-up-env/first-assignment.html">
     37.3. First assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../set-up-env/second-assignment.html">
     37.4. Second assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../project-plan-template.html">
     37.5. Project Plan​ Template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prerequisites/python-programming-introduction.html">
     37.6. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prerequisites/python-programming-basics.html">
     37.7. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prerequisites/python-programming-advanced.html">
     37.8. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/analyzing-text-about-data-science.html">
     37.9. Analyzing text about Data Science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/data-science-scenarios.html">
     37.10. Data Science scenarios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/write-a-data-ethics-case-study.html">
     37.11. Write a data ethics case study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/lines-scatters-and-bars.html">
     37.12. Lines, scatters and bars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/apply-your-skills.html">
     37.13. Apply your skills
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/try-it-in-excel.html">
     37.14. Try it in Excel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/dive-into-the-beehive.html">
     37.15. Dive into the beehive
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/build-your-own-custom-vis.html">
     37.16. Build your own custom vis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/classifying-datasets.html">
     37.17. Classifying datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/small-diabetes-study.html">
     37.18. Small diabetes study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/introduction-to-statistics-and-probability.html">
     37.19. Introduction to probability and statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/displaying-airport-data.html">
     37.20. Displaying airport data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/soda-profits.html">
     37.21. Soda profits
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/analyzing-COVID-19-papers.html">
     37.22. Analyzing COVID-19 papers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/estimation-of-COVID-19-pandemic.html">
     37.23. Estimation of COVID-19 pandemic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/data-processing-in-python.html">
     37.24. Data processing in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/evaluating-data-from-a-form.html">
     37.25. Evaluating data from a form
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/data-preparation.html">
     37.26. Data preparation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/analyzing-data.html">
     37.27. Analyzing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/nyc-taxi-data-in-winter-and-summer.html">
     37.28. NYC taxi data in winter and summer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/matplotlib-applied.html">
     37.29. Matplotlib applied
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/tell-a-story.html">
     37.35. Tell a story
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/explore-a-planetary-computer-dataset.html">
     37.36. Explore a planetary computer dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/exploring-for-anwser.html">
     37.37. Exploring for answers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/market-research.html">
     37.38. Market research
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/low-code-no-code-data-science-project-on-azure-ml.html">
     37.39. Low code/no code Data Science project on Azure ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/data-science-project-using-azure-ml-sdk.html">
     37.40. Data Science project using Azure ML SDK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data-science/data-science-in-the-cloud-the-azure-ml-sdk-way.html">
     37.41. Data Science in the cloud: The “Azure ML SDK” way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-overview-iris.html">
     37.42. Machine Learning overview - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-overview-mnist-digits.html">
     37.43. Machine Learning overview - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression-with-scikit-learn.html">
     37.44. Regression with Scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-linear-regression-1.html">
     37.45. ML linear regression - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-linear-regression-2.html">
     37.46. ML linear regression - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-regression-metrics.html">
     37.47. Linear Regression Metrics
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     37.48. Loss Function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient-descent.html">
     37.49. Gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-logistic-regression-1.html">
     37.51. ML logistic regression - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-logistic-regression-2.html">
     37.52. ML logistic regression - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-neural-network-1.html">
     37.53. ML neural network - Assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression-tools.html">
     37.54. Regression tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../managing-data.html">
     37.55. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exploring-visualizations.html">
     37.56. Exploring visualizations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../try-a-different-model.html">
     37.57. Try a different model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../create-a-regression-model.html">
     37.58. Create a regression model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../linear-and-polynomial-regression.html">
     37.59. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../retrying-some-regression.html">
     37.60. Retrying some regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pumpkin-varieties-and-color.html">
     37.61. Pumpkin varieties and color
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../delicious-asian-and-indian-cuisines.html">
     37.62. Delicious asian and indian cuisines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../explore-classification-methods.html">
     37.63. Explore classification methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../linear-regression-implementation-from-scratch.html">
     37.64. Linear Regression Implementation from Scratch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/kernel-method/kernel-method-assignment-1.html">
     37.65. Kernel method assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/kernel-method/support_vector_machines_for_regression.html">
     37.66. Support Vector Machines (SVM) - Intro and SVM for Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/kernel-method/support_vector_machines_for_classification.html">
     37.67. Support Vector Machines (SVM) - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/kernel-method/decision_trees_for_regression.html">
     37.68. Decision Trees - Intro and Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/kernel-method/decision_trees_for_classification.html">
     37.69. Decision Trees - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/model-selection/model-selection-assignment-1.html">
     37.70. Model selection assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/model-selection/learning-curve-to-identify-overfit-underfit.html">
     37.71. Learning Curve To Identify Overfit &amp; Underfit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/model-selection/dropout-and-batch-normalization.html">
     37.72. Dropout and Batch Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/model-selection/lasso-and-ridge-regression.html">
     37.73. Lasso and Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/model-selection/regularized-linear-models.html">
     37.74. Regularized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/ensemble-learning/random-forests-intro-and-regression.html">
     37.75. Random forests intro and regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/ensemble-learning/random-forests-for-classification.html">
     37.76. Random forests for classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/ensemble-learning/beyond-random-forests-more-ensemble-models.html">
     37.77. Beyond random forests: more ensemble models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/ensemble-learning/decision-trees.html">
     37.78. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/gradient-boosting/hyperparameter-tuning-gradient-boosting.html">
     37.79. Hyperparameter tuning gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/gradient-boosting/gradient-boosting-assignment.html">
     37.80. Gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/gradient-boosting/boosting-with-tuning.html">
     37.81. Boosting with tuning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-advanced/ensemble-learning/random-forest-classifier-feature-importance.html">
     37.82. Random Forest Classifier with Feature Importance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../machine-learning-productionization/data-engineering.html">
     37.84. Data engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../machine-learning-productionization/counterintuitive-challenges-in-ml-debugging.html">
     37.85. Counterintuitive Challenges in ML Debugging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../machine-learning-productionization/debugging-in-classification.html">
     37.86. Case Study: Debugging in Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../machine-learning-productionization/debugging-in-regression.html">
     37.87. Case Study: Debugging in Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../study-the-solvers.html">
     37.88. Study the solvers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../build-classification-models.html">
     37.89. Build classification models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../build-classification-model.html">
     37.90. Build Classification Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../parameter-play.html">
     37.91. Parameter play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/cnn/how-to-choose-cnn-architecture-mnist.html">
     37.92. How to choose cnn architecture mnist
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/cnn/sign-language-digits-classification-with-cnn.html">
     37.94. Sign Language Digits Classification with CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/cnn/object-recognition-in-images-using-cnn.html">
     37.96. Object Recognition in Images using CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/tensorflow/intro_to_tensorflow_for_deeplearning.html">
     37.97. Intro to TensorFlow for Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/lstm/bitcoin-lstm-model-with-tweet-volume-and-sentiment.html">
     37.99. Bitcoin LSTM Model with Tweet Volume and Sentiment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/rnn/google-stock-price-prediction-rnn.html">
     37.101. Google Stock Price Prediction RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/autoencoder/autoencoder.html">
     37.103. Intro to Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/autoencoder/base-denoising-autoencoder-dimension-reduction.html">
     37.104. Base/Denoising Autoencoder &amp; Dimension Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/autoencoder/variational-autoencoder-and-faces-generation.html">
     37.105. Fun with Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/time-series-forecasting-assignment.html">
     37.106. Time Series Forecasting Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/nn-for-classification-assignment.html">
     37.108. Neural Networks for Classification with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/nn-classify-15-fruits-assignment.html">
     37.109. NN Classify 15 Fruits Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/dqn/dqn-on-foreign-exchange-market.html">
     37.114. DQN On Foreign Exchange Market
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/gan/art-by-gan.html">
     37.115. Art by gan
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/gan/gan-introduction.html">
     37.117. Generative Adversarial Networks (GANs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../deep-learning/overview/basic-classification-classify-images-of-clothing.html">
     37.118. Basic classification: Classify images of clothing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../slides/introduction.html">
   38. Slides
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/python-programming/python-programming-introduction.html">
     38.1. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/python-programming/python-programming-basics.html">
     38.2. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/python-programming/python-programming-advanced.html">
     38.3. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/data-science-introduction.html">
     38.4. Data Science introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/relational-vs-non-relational-database.html">
     38.5. Relational vs. non-relational database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/numpy-and-pandas.html">
     38.6. NumPy and Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/data-visualization.html">
     38.7. Data visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/data-science-lifecycle.html">
     38.8. Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/data-science-in-the-cloud.html">
     38.9. Data Science in the cloud
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/data-science/data-science-in-real-world.html">
     38.10. Data Science in real world
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/ml-overview.html">
     38.11. Machine Learning overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/linear-regression.html">
     38.12. Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/logistic-regression.html">
     38.13. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/logistic-regression-condensed.html">
     38.14. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/neural-network.html">
     38.15. Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-fundamentals/build-an-ml-web-app.html">
     38.16. Build an machine learning web application
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-advanced/unsupervised-learning.html">
     38.17. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-advanced/kernel-method.html">
     38.18. Kernel method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/ml-advanced/model-selection.html">
     38.19. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/deep-learning/cnn.html">
     38.20. Convolutional Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../slides/deep-learning/gan.html">
     38.21. Generative Adversarial Network
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ocademy-ai/machine-learning/release?urlpath=lab/tree/open-machine-learning-jupyter-book/assignments/ml-fundamentals/linear-regression/loss-function.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/ocademy-ai/machine-learning/blob/release/open-machine-learning-jupyter-book/assignments/ml-fundamentals/linear-regression/loss-function.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning//issues/new?title=Issue%20on%20page%20%2Fassignments/ml-fundamentals/linear-regression/loss-function.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/edit/release/open-machine-learning-jupyter-book/assignments/ml-fundamentals/linear-regression/loss-function.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/assignments/ml-fundamentals/linear-regression/loss-function.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective-of-this-section">
   37.48.1. Objective of this section
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#concept-of-loss-function">
   37.48.2. Concept of loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#difference-between-a-loss-function-and-a-cost-function">
   37.48.3. Difference between a Loss Function and a Cost Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-of-loss-functions">
   37.48.4. Classification of loss functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-losses">
     37.48.4.1. Regression Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-losses">
     37.48.4.2. Classification Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-losses">
     37.48.4.3. Structured Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-losses">
     37.48.4.4. Regularization Losses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-risk-and-structural-risk">
   37.48.5. Empirical Risk and Structural Risk
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     37.48.5.1. Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-detaphor">
     37.48.5.2. A Detaphor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-explanation">
     37.48.5.3. Mathematical Explanation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function-and-objective-function">
     37.48.5.4. Cost Function and Objective Function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-loss-functions">
   37.48.6. Common Loss Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-loss-functions">
     37.48.6.1. Regression Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-loss-functions">
     37.48.6.2. Classification Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-loss-functions">
     37.48.6.3. Structured Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization">
     37.48.6.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   37.48.7. Conclusion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Loss Function</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective-of-this-section">
   37.48.1. Objective of this section
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#concept-of-loss-function">
   37.48.2. Concept of loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#difference-between-a-loss-function-and-a-cost-function">
   37.48.3. Difference between a Loss Function and a Cost Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-of-loss-functions">
   37.48.4. Classification of loss functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-losses">
     37.48.4.1. Regression Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-losses">
     37.48.4.2. Classification Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-losses">
     37.48.4.3. Structured Losses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-losses">
     37.48.4.4. Regularization Losses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-risk-and-structural-risk">
   37.48.5. Empirical Risk and Structural Risk
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     37.48.5.1. Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-detaphor">
     37.48.5.2. A Detaphor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-explanation">
     37.48.5.3. Mathematical Explanation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function-and-objective-function">
     37.48.5.4. Cost Function and Objective Function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-loss-functions">
   37.48.6. Common Loss Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-loss-functions">
     37.48.6.1. Regression Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-loss-functions">
     37.48.6.2. Classification Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-loss-functions">
     37.48.6.3. Structured Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization">
     37.48.6.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   37.48.7. Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="loss-function">
<h1><span class="section-number">37.48. </span>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this headline">#</a></h1>
<section id="objective-of-this-section">
<h2><span class="section-number">37.48.1. </span>Objective of this section<a class="headerlink" href="#objective-of-this-section" title="Permalink to this headline">#</a></h2>
<p>We have already learned math and code for “Gradient Descent”, as well as other optimization techniques.</p>
<p>In this section, we will learn more about loss functions.</p>
<p>As a learner, you can focus on learning the L1, L2 loss, and Classification Losses in the regression loss function in this section. And learn about other loss functions.</p>
</section>
<section id="concept-of-loss-function">
<h2><span class="section-number">37.48.2. </span>Concept of loss function<a class="headerlink" href="#concept-of-loss-function" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A loss function gauges the disparity between the model’s predictions and the actual values. Simply put, it indicates how “off” our model is. By optimizing this function, our objective is to identify parameters that bring the model’s predictions as close as possible to the true values.</p></li>
<li><p>The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.
— Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville</p></li>
</ul>
</section>
<section id="difference-between-a-loss-function-and-a-cost-function">
<h2><span class="section-number">37.48.3. </span>Difference between a Loss Function and a Cost Function<a class="headerlink" href="#difference-between-a-loss-function-and-a-cost-function" title="Permalink to this headline">#</a></h2>
<p>A loss function evaluates the error for a single training example, and it is occasionally referred to as an error function. In contrast, a cost function represents the <strong>average loss</strong> across the entire training dataset. Optimization strategies are designed to minimize this cost function.</p>
<p>For a simple sample:</p>
<p>The corresponding cost function of L1 Loss is the Mean of these Squared Errors (MSE).
You can see the difference of <a class="reference external" href="#regression-loss-functions">Mathematical Expression</a></p>
<p>However, these terms are frequently used interchangeably in practical settings, they aren’t precisely equivalent. From a definitional standpoint, the cost function represents an aggregation or average of the loss functions.</p>
</section>
<section id="classification-of-loss-functions">
<h2><span class="section-number">37.48.4. </span>Classification of loss functions<a class="headerlink" href="#classification-of-loss-functions" title="Permalink to this headline">#</a></h2>
<section id="regression-losses">
<h3><span class="section-number">37.48.4.1. </span>Regression Losses<a class="headerlink" href="#regression-losses" title="Permalink to this headline">#</a></h3>
<p>These are employed when the objective is to predict a continuous outcome.</p>
<ul class="simple">
<li><p>Mean Squared Error (MSE): Measures the average squared discrepancies between predictions and actual values, emphasizing larger errors.</p></li>
<li><p>Mean Absolute Error (MAE): Calculates the average of absolute differences between predicted outcomes and actual observations, offering a linear penalty for each deviation.</p></li>
<li><p>Huber Loss: A hybrid loss that’s quadratic for small differences and linear for large ones, providing resilience against outliers.</p></li>
<li><p>L1 Loss: Directly reflects the absolute discrepancies between predictions and real values, synonymous with MAE.</p></li>
<li><p>L2 Loss: Highlights squared differences between predictions and actuals, equivalent to MSE.</p></li>
<li><p>Smooth L1 Loss: An amalgamation of L1 and L2 losses, it provides a balance in handling both minor and major deviations.</p></li>
</ul>
</section>
<section id="classification-losses">
<h3><span class="section-number">37.48.4.2. </span>Classification Losses<a class="headerlink" href="#classification-losses" title="Permalink to this headline">#</a></h3>
<p>Utilized for tasks requiring the prediction of discrete categories.</p>
<ul class="simple">
<li><p>Cross Entropy Loss: Quantifies the dissimilarity between the predicted probability distribution and the actual class distribution.</p></li>
<li><p>Hinge Loss: A staple for Support Vector Machines (SVMs), it strives to categorize data by maximizing the decision boundary between classes.</p></li>
<li><p><strong>Binary Cross Entropy Loss(Log Loss):</strong> It is intended for use with binary classification where the target values are in the set {0, 1}. It is a special case of Cross Entropy Loss, specially used for binary classification problems.</p></li>
<li><p><strong>Multi-Class Cross-Entropy Loss:</strong> In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer <a class="reference external" href="http://value.It">value.It</a> is an extension of Cross Entropy Loss and is used for multi-classification problems.</p></li>
</ul>
</section>
<section id="structured-losses">
<h3><span class="section-number">37.48.4.3. </span>Structured Losses<a class="headerlink" href="#structured-losses" title="Permalink to this headline">#</a></h3>
<p>Tailored for intricate tasks involving structured data patterns.</p>
<ul class="simple">
<li><p>Sequence Generation Loss: Emblematic examples include the CTC (Connectionist Temporal Classification) designed for undertakings such as speech and text identification.</p></li>
<li><p>Image Segmentation Loss: Noteworthy instances encompass the Dice loss and the IoU (Intersection over Union) loss.</p></li>
</ul>
</section>
<section id="regularization-losses">
<h3><span class="section-number">37.48.4.4. </span>Regularization Losses<a class="headerlink" href="#regularization-losses" title="Permalink to this headline">#</a></h3>
<p>Rather than directly influencing the model’s predictions, these losses are integrated into the objective function to counteract excessive model complexity.</p>
<ul class="simple">
<li><p>L1 Regularization (Lasso): Enforces sparsity by compelling certain model coefficients to be exactly zero.</p></li>
<li><p>L2 Regularization (Ridge): Curbs the unchecked growth of model parameters without nullifying them, ensuring the model remains generalized without undue complexity.</p></li>
</ul>
</section>
</section>
<section id="empirical-risk-and-structural-risk">
<h2><span class="section-number">37.48.5. </span>Empirical Risk and Structural Risk<a class="headerlink" href="#empirical-risk-and-structural-risk" title="Permalink to this headline">#</a></h2>
<section id="definition">
<h3><span class="section-number">37.48.5.1. </span>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">#</a></h3>
<p>Perhaps you’ve heard of these two concepts before. In the realms of machine learning and statistics, the concepts of empirical risk and structural risk are intricately tied to loss functions. <strong>However, these terms aren’t directly categories of loss functions perse.</strong> Let’s first clarify these concepts:</p>
<ol class="simple">
<li><p><strong>Empirical Risk:</strong> Refers to the average loss of a model over a given dataset. Minimizing empirical risk focuses on reducing errors explicitly on the training data.</p></li>
<li><p><strong>Structural Risk:</strong> Introduces a regularization term in addition to empirical risk, aiming to prevent overfitting. Minimizing structural risk strikes a balance between the empirical risk and the complexity of the model.</p></li>
</ol>
<p>Given these definitions:</p>
<ul class="simple">
<li><p><strong>Empirical Risk:</strong> Loss functions directly related to dataset performance fall under this category. From the ones been listed, regression losses (e.g., MSE, MAE, Huber Loss, L1 Loss, L2 Loss, Smooth L1 Loss), classification losses (e.g., Cross Entropy Loss, Hinge Loss, Log Loss), and structured losses (e.g., CTC or Image Segmentation Loss) can be seen as manifestations of empirical risk.</p></li>
<li><p><strong>Structural Risk:</strong> Regularization losses, like L1 and L2 regularization, form part of structural risk. They don’t measure the model’s performance on the data directly but rather serve to rein in model complexity.</p></li>
</ul>
</section>
<section id="a-detaphor">
<h3><span class="section-number">37.48.5.2. </span>A Detaphor<a class="headerlink" href="#a-detaphor" title="Permalink to this headline">#</a></h3>
<p>Maybe it’s still abtract. So, now imagine you’re a tailor trying to make a dress for a client.</p>
<ul class="simple">
<li><p><strong>Empirical Risk:</strong> This is like ensuring the dress fits the client perfectly based on a single fitting session. You measure every contour and make the dress to match those exact measurements. The dress is a perfect fit for the client on that particular day.</p></li>
</ul>
<p>However, what if the client gains or loses a little weight or wants to move more comfortably? A dress tailored too tightly to the exact measurements might not be very adaptable or comfortable in various situations.</p>
<ul class="simple">
<li><p><strong>Structural Risk:</strong> Now, consider that you decide to allow a bit more flexibility in the dress. You make it slightly adjustable, perhaps with some elastic portions. This way, even if the client’s measurements change a bit, the dress will still fit comfortably. You’re sacrificing a tiny bit of the “perfect” fit for the adaptability and general comfort.</p></li>
</ul>
<p>In the context of machine learning:</p>
<p>Relying solely on <strong>Empirical Risk</strong> would be like fitting the dress exactly to the client’s measurements, risking overfitting. If the data changes slightly, the model might perform poorly.</p>
<p>Factoring in <strong>Structural Risk</strong> ensures the model isn’t overly tailored to the training data and can generalize well to new, unseen data. It’s about ensuring a balance between a perfect fit and adaptability.</p>
</section>
<section id="mathematical-explanation">
<h3><span class="section-number">37.48.5.3. </span>Mathematical Explanation<a class="headerlink" href="#mathematical-explanation" title="Permalink to this headline">#</a></h3>
<p>Now you have a general understanding of the meaning of empirical risk and structural risk. Let’s delve into a more mathematical perspective:</p>
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> comprising input-output pairs <span class="math notranslate nohighlight">\((x_1, y_1)\)</span>, <span class="math notranslate nohighlight">\((x_2, y_2)\)</span>, … <span class="math notranslate nohighlight">\((x_n, y_n)\)</span> and a model <span class="math notranslate nohighlight">\(f\)</span> parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>, the empirical risk and structural risk can be formally defined as follows:</p>
<p><strong>Empirical Risk(Cost Function):</strong>
$<span class="math notranslate nohighlight">\(
R_{emp}(f) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, f(x_i; \theta))
\)</span>$
Where:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(L\)</span> is the loss function</strong>, measuring the discrepancy between the predicted value <span class="math notranslate nohighlight">\(f(x_i; \theta)\)</span> and the actual output <span class="math notranslate nohighlight">\(y_i\)</span>.</p></li>
</ul>
<p>Empirical risk quantifies how well the model fits the given dataset, representing the average loss of the model on the training data.</p>
<p><strong>Structural Risk(Objective Function):</strong>
$<span class="math notranslate nohighlight">\(
R_{struc}(f) = R_{emp}(f) + \lambda R_{reg}(\theta)
\)</span>$
Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R_{reg}(\theta)\)</span> is the regularization term, penalizing the complexity of the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is a regularization coefficient determining the weight of the regularization term relative to the empirical risk.</p></li>
</ul>
<p>Structural risk is a combination of the empirical risk and a penalty for model complexity. It strikes a balance between fitting the training data (empirical risk) and ensuring the model isn’t overly complex (which can lead to overfitting).</p>
<p><strong>Differences and Relations:</strong></p>
<ol class="simple">
<li><p><strong>Empirical Risk</strong> focuses solely on minimizing the error on the training data without considering model complexity or how it generalizes to unseen data.</p></li>
<li><p><strong>Structural Risk</strong> takes into account both the empirical risk and the complexity of the model. By introducing a regularization term, it ensures that the model doesn’t become overly complex and overfit the training data. Thus, it balances performance on training data with generalization to new data.</p></li>
</ol>
<p>In essence, while empirical risk aims for performance on the current dataset, structural risk aims for good performance on new data by penalizing overly complex models.</p>
</section>
<section id="cost-function-and-objective-function">
<h3><span class="section-number">37.48.5.4. </span>Cost Function and Objective Function<a class="headerlink" href="#cost-function-and-objective-function" title="Permalink to this headline">#</a></h3>
<p>The empirical risk and cost functions are in many cases the same and represent the average loss on the training data.</p>
<p>Structural risk is often viewed as an objective function, especially when regularization is considered. But the term “objective function” is broader and is not limited to structural risk but can also include other optimization objectives.</p>
</section>
</section>
<section id="common-loss-functions">
<h2><span class="section-number">37.48.6. </span>Common Loss Functions<a class="headerlink" href="#common-loss-functions" title="Permalink to this headline">#</a></h2>
<section id="regression-loss-functions">
<h3><span class="section-number">37.48.6.1. </span>Regression Loss Functions<a class="headerlink" href="#regression-loss-functions" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p><strong>Mean Squared Error, MSE</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 
\]</div>
<p>Where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual value and <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MSE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Mean Absolute Error, MAE</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MAE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Huber Loss</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{\delta}(y, \hat{y}) = \begin{cases} 
        \frac{1}{2}(y - \hat{y})^2 &amp; \text{if } |y - \hat{y}| \leq \delta \\
        \delta |y - \hat{y}| - \frac{1}{2}\delta^2 &amp; \text{otherwise}
        \end{cases}
\end{split}\]</div>
<p>Positioned between MSE and MAE, this loss function offers robustness against outliers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Huber</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>L1 Loss</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L = ( y - f(x) )^2
\]</div>
<p>Corresponds to MAE.</p>
<ol class="simple">
<li><p><strong>L2 Loss</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L = | y - f(x) |
\]</div>
<p>Corresponds to MSE.</p>
</section>
<section id="classification-loss-functions">
<h3><span class="section-number">37.48.6.2. </span>Classification Loss Functions<a class="headerlink" href="#classification-loss-functions" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p><strong>Cross Entropy Loss</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L(y, p) = - \sum_{i=1}^{C} y_i \log(p_i)
\]</div>
<p>Where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual label (0 or 1) and <span class="math notranslate nohighlight">\(p_i\)</span> is the predicted probability for the respective class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Hinge Loss</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})
\]</div>
<p>Primarily used for Support Vector Machines, but it can also be employed for other classification tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># binary class labels in {-1, 1}</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">])</span>  <span class="c1"># raw model outputs</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Hinge</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Binary Cross Entropy(Log Loss)</strong></p></li>
</ol>
<p>Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.</p>
<p>Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.</p>
<p>This YouTube video by Andrew Ng explains very well Binary Cross Entropy Loss (make sure that you have access to YouTube for this web page to render correctly):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &lt;iframe src=&quot;https://www.youtube.com/embed/SHEPb1JHw5o&quot; allowfullscreen&gt;&lt;/iframe&gt;</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Multi-Class Cross-Entropy Loss</strong></p></li>
</ol>
<p>Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.</p>
<p>Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>

<span class="c1"># Mock predicted probabilities from a model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Multi-Class Cross-Entropy Loss:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="structured-loss-functions">
<h3><span class="section-number">37.48.6.3. </span>Structured Loss Functions<a class="headerlink" href="#structured-loss-functions" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p><strong>CTC Loss (Connectionist Temporal Classification)</strong></p></li>
</ol>
<p>Used for sequence-to-sequence problems, like speech recognition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  <span class="c1"># (batch, timesteps)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]])</span>  <span class="c1"># (batch, timesteps, num_classes)</span>
<span class="n">logit_length</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">label_length</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">ctc_batch_cost</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">logit_length</span><span class="p">,</span> <span class="n">label_length</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>Dice Loss, IoU Loss</strong></p></li>
</ol>
<p>Used for image segmentation tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dice_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">+</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">numerator</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">denominator</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">dice_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">iou_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">union</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">-</span> <span class="n">intersection</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">-</span> <span class="p">(</span><span class="n">intersection</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">union</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># For simplicity, using 2D tensors. Typically, these are images (3D tensors).</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">iou_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># Add batch dimension</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="regularization">
<h3><span class="section-number">37.48.6.4. </span>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p><strong>L1 Regularization (Lasso)</strong></p></li>
</ol>
<p>Produces sparse model parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.regularizers</span> <span class="kn">import</span> <span class="n">l1</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p><strong>L2 Regularization (Ridge)</strong></p></li>
</ol>
<p>Prevents model parameters from becoming too large but doesn’t force them to become exactly zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.regularizers</span> <span class="kn">import</span> <span class="n">l2</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion">
<h2><span class="section-number">37.48.7. </span>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>Loss functions hold a pivotal role in machine learning. By minimizing the loss, we enhance the accuracy of our model’s predictions. A deep understanding of various loss functions aids in selecting the most appropriate optimization technique for specific challenges.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ocademy-ai/machine-learning",
            ref: "release",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./assignments/ml-fundamentals/linear-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="linear-regression-metrics.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">37.47. </span>Linear Regression Metrics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gradient-descent.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">37.49. </span>Gradient descent</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ocademy<br/>
  
      &copy; Copyright 2022-2023.<br/>
    <div class="extra_footer">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> Text content of this work is licensed under the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>